+++ 
title = "Link List :: 2025-05-25"
description = ""
slug = ""
tags = ["links"]
categories = ["linklist"]
externalLink = ""
series = []
+++

## https://github.com/rneatherway/gh-slack

```
* This project provides a means of archiving a Slack conversation or thread as markdown.
* For convenience, it is installable as a gh extension: `gh extension install https://github.com/rneatherway/gh-slack` and then upgrade with `gh extension upgrade gh-slack`
```

## https://github.com/automata/aicodeguide?tab=readme-ov-file#vibe

This is an extensive guide on how to use Large Language Models (LLMs) for coding purposes. Here's a summary of the main points:

**Getting started with LLMs**

1. Choose an LLM tool, such as Cursor Agent or Aider.
2. Set up your development environment and project structure.
3. Create a PRD (Product Requirements Document) to guide your LLM's implementation.

**LLM Training and Guidelines**

1. Train your LLM using a starter kit or existing codebase.
2. Use tools like repomix, files-to-prompt, or indexing features to optimize your LLM's performance.
3. Prompt-engineer your design sequence to produce well-structured code.

**Tips for Effective LLM Coding**

1. Separate engine and policy code, and prompt the LLM to guide it.
2. Include rules in your project that prevent layering violations.
3. Start with core functionality and organize your code structure before exposing it as a library or API.
4. Write tests and use TDD (Test-Driven Development) to create test-driven skeletons.

**Testing and Validation**

1. Regularly check the generated code for correctness and consistency.
2. Use tests to validate the LLM's output and ensure that it meets your requirements.

**Tools and Resources**

1. Keep an updated list of main tools around using AI for coding.
2. Check out the work of influential individuals, such as Andrej Karpathy, Geoffrey Huntley, and Eric S. Raymond.

## https://cloud.google.com/run/docs/tips/python

```
* Optimizations for Python-based Cloud Run services include handling concurrent requests and reducing response latency using connection pooling and batching non-critical functions.
* Optimize the container image by:
    + Only putting necessary components into the container at runtime
    + Using a smaller base image (e.g. `python:slim` or `distroless`)
* Minimize the container image by:
    + Moving large files outside of the container to Cloud Storage
    + Adding specific packages back in using `RUN apt install`
* Optimize the WSGI server by:
    + Using a smaller number of workers and threads (e.g. with gunicorn's `--preload` setting)
    + Setting up custom startup probes for your service
    + Considering alternative WSGI servers like uwsgi, uvicorn, or waitress
* Optimize applications by:
    + Reducing threads to optimize memory usage
    + Decreasing the number of startup tasks to reduce cold start latency
* Consider using instance-based billing to run background activities outside of requests.
```

## https://github.com/HichemTab-tech/JetTreeMark

```
- Generate a clean tree view of any selected folder
- One-click copy to clipboard
- Flexible node control with context menu options:
  - Check only folders or only files
  - Check nodes without affecting their children
  - Expand or collapse all nodes
- Ultra lightweight and seamless integration with the IDE
```

## https://github.com/rockbite/localforge

```
* Local coding agent with neat UI
* Runs in a local web UI and autonomously works on your files alongside you.
* `npm i -g @rockbite/localforge` to install globally
* Install dependencies: `npm install`
* Start development server: `npm run start`
* Main guide: https://localforge.dev/how-to
```

## https://arxiv.org/abs/2504.16736

```
* A Survey of AI Agent Protocols
* Abstract:The rapid development of large language models (LLMs) has led to the widespread deployment of LLM agents across diverse industries.
* However, as more LLM agents are deployed, a major issue has emerged: there is no standard way for these agents to communicate with external tools or data sources.
* This lack of standardized protocols makes it difficult for agents to work together or scale effectively.
* A unified communication protocol for LLM agents could change this.
* It would allow agents and tools to interact more smoothly, encourage collaboration, and triggering the formation of collective intelligence.
* In this paper, we provide the first comprehensive analysis of existing agent protocols.
* We propose a systematic two-dimensional classification that differentiates context-oriented versus inter-agent protocols and general-purpose versus domain-specific protocols.
* Additionally, we conduct a comparative performance analysis of these protocols across key dimensions such as security, scalability, and latency.
* Finally, we explore the future landscape of agent protocols by identifying critical research directions and characteristics necessary for next-generation protocols.
* These characteristics include adaptability, privacy preservation, and group-based interaction.
* We also identify trends toward layered architectures and collective intelligence infrastructures.
* We expect this work to serve as a practical reference for both researchers and engineers seeking to design, evaluate, or integrate robust communication infrastructures for intelligent agents.
```

## https://github.com/n8n-io/self-hosted-ai-starter-kit

```
* Self-hosted AI Starter Kit is an open-source Docker Compose template designed to swiftly initialize a comprehensive local AI and low-code development environment.
* Curated by https://github.com/n8n-io, it combines the self-hosted n8n platform with a curated list of compatible AI products and components.
* Key components:
    * Self-hosted n8n - Low-code platform with over 400 integrations and advanced AI components
    * Ollama - Cross-platform LLM platform to install and run the latest local LLMs
    * Qdrant - Open-source, high performance vector store with a comprehensive API
    * PostgreSQL - Workhorse of the Data Engineering world, handles large amounts of data safely.
* Features:
    * AI Agents for scheduling appointments
    * Summarize Company PDFs securely without data leaks
    * Smarter Slack Bots for enhanced company communications and IT operations
    * Private Financial Document Analysis at minimal cost
* Installation steps:
    * Clone the repository and navigate to the directory
    * Run `docker compose --profile gpu-nvidia up` (or `--profile cpu up`) depending on GPU availability
    * Configure OLLAMA_HOST environment variable if running locally
* Getting started:
    * Open http://localhost:5678/ in browser to set up n8n
    * Open included workflow and start running the workflow
    * Use Ollama node for language model and Qdrant as vector store
* Customization and optimization:
    * Note that this starter kit is not fully optimized for production environments
    * Can be customized to meet specific needs
* Resources:
    * n8n AI template gallery for more workflow ideas
    * Support page for troubleshooting and community discussion
```

## https://github.com/shobrook/suss

```
* Find bugs in local code changes with a codebase-aware AI agent, analyzing diff between local and remote branches.
* Run `suss` in terminal to get bug report in under 1 minute.
* Works by exploring codebase context for each code change and evaluating it using a reasoning model.
* Can catch complex, multi-file bugs and is useful as a quick code review before pushing code.
* Install with `pipx install suss`.
* Choose LLM provider (e.g. OpenAI, Anthropic) and add API key to environment variables.
* Use other models or local ones, including Cohere reranker model.
* Boost performance using Cohere API key.
* Run `suss` in root directory of codebase by default analyzing new or modified files.
* Run `suss --file="path/to/file.py"` for specific file analysis.
```

## https://blog.prateekjain.dev/a-step-by-step-guide-to-docker-image-optimisation-reduce-size-by-over-95-d90bcab3819d

```
* A Step-by-Step Guide to Docker Image Optimisation: Reduce Size by Over 95%
* Deployed a Node.js app to production using Docker, resulting in a 1.2 GB final image size
* Oversized images slowed down deployments, hiked up costs, and increased attack surface
* Techniques found to shrink Docker images by up to 95% through trial-and-error
* Image size affects application behavior in production:
    * Faster Deployments: Smaller images move through CI/CD pipelines faster
    * Reduced Costs: Lean images consume less storage and resources
    * Impact on Infrastructure Costs
```

## https://github.com/plasma-umass/ChatDBG

```
The ChatDBG documentation provides an overview of how to use the tool for debugging various programming languages, including Rust.
```

## https://github.com/aperoc/toolkami

```
Minimal AI agent framework that just works with only seven tools.
* Seven tools are required: 
  * Minimal AI agent
  * Turbo mode
  * Hot-reloading
  * Devcontainer with UV installation instructions
  * Servers/.env and clients/.env files for credentials
  * MCP server start command
  * MCP client start command
  * Command to clear message history

* MCP tool:
  * Minimal implementation of AlphaEvolve using the framework
  * Diff-fenced editing
  * Credits to https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/
  
* Additional features and tools:
  * System prompt guidelines
  * Single file project templates
  * OpenAI editing format
  * Customisable sharp tool (with guardrails to be implemented over time)
```

## https://github.com/maverickg59/sushimcp

```
* SushiMCP is a model context protocol server designed to assist developers with delivering context to their AI IDE's.
* It improves the performance of base and premium LLM models when generating code.
* The easiest way to get started is by registering SushiMCP with your client using the default configuration.
* The default configuration uses:
  * `npx` command
  * `@chriswhiterocks/sushimcp` module
  * `--llms-txt-source` and `--openapi-spec-source` flags
* Visit the [SushiMCP Docs](link) for more information on advanced configuration.
* The project is licensed under the AGPL-3.0-or-later license.
* See the [license.txt](file) for details about the license.
```

## https://old.reddit.com/r/LocalLLaMA/comments/1khjrtj/building_llm_workflows_some_observations/

```
* Decomposing tasks into smallest steps and prompt chaining works better than using a single prompt with CoT
* Using XML tags for system prompt, prompt etc is preferred over JSON structure (IMO)
* LLM should be reminded of its role as a semantic parser, without introducing data from its own "knowledge"
* NLTK, SpaCY, FlairNLP are good for verifying output of an LLM independently
* ModernBERT classifiers are often just as good as LLMs for small tasks
* Fine-tuned BERT-style classifiers are usually better than LLM for focused narrow tasks
* LLM-as-judge and confidence scoring is unreliable, especially without "grounding" parameters
* Scoring on vague parameters like helpfulness is useless - requires grounding or fine-tuning model
* Agentic loops require setting conditions for LLM to exit loop, which is unreliable
* Performance usually degrades past 4k tokens (input context window)
* 32B models are good enough and reliable enough for most tasks
* Structured CoT with headings and bullet points is often better than unstructured CoT tokens
* Self-consistency helps but requires running each prompt multiple times, forcing smaller models and prompts
* Writing own CoT is better than relying on reasoning model for ideas synthesis
* Long-term plan is to fine-tune everything, starting with large API-based model and few-shot examples
* Fine-tuning datasets should be balanced by setting up a categorization system/orthogonal taxonomy
```

## https://github.com/marcboeker/gmail-to-sqlite

```
Index your Gmail account to a SQLite DB and play with the data.
```

## https://www.pewresearch.org/short-reads/2024/03/05/online-opt-in-polls-can-produce-misleading-results-especially-for-young-people-and-hispanic-adults/

```
* At Pew Research Center, we've analyzed the strengths and weaknesses of polling methods, including online "opt-in" sampling, which can struggle to measure attitudes among young people and Hispanics due to the presence of "bogus respondents."
* Bogus respondents attempt to complete surveys with minimal effort to earn rewards and may overreport rare attitudes or behaviors.
* Studies have shown that bogus respondents are more concentrated in estimates for adults under 30 and Hispanic adults, leading to unreliable estimates for these groups.
* A recent online opt-in survey reported a startling finding about Holocaust denial among young Americans, which we replicated using a probability-based survey panel.
* Our findings showed that the original opt-in poll's estimates were significantly different from ours, suggesting that bogus respondents may have driven the results.
* The replication also found substantial differences in support for legal abortion between the two surveys, supporting our expectation that bogus respondents would be responsible for these discrepancies.
* The study highlights the importance of using rigorous survey methods to avoid inadvertently misleading the public, particularly when studying complex and sensitive topics like antisemitism and the Middle East.
```

## https://github.com/collidingScopes/shape-creator-tutorial

```
* The demo uses:
  + Three.js for 3D rendering
  + MediaPipe for hand tracking and gesture recognition
  + HTML5 Canvas for visual feedback
  + JavaScript for real-time interaction
```

## https://github.com/sisig-ai/doctor

```
- **Overview**: Tool for discovering, crawling and indexing web sites to be exposed as an MCP server for LLM agents
- **Doctor's capabilities**:
  * Crawling web pages using crawl4ai with hierarchy tracking
  * Chunking text with LangChain
  * Creating embeddings with OpenAI via litellm
  * Storing data in DuckDB with vector search support
  * Exposing search functionality via a FastAPI web service
- **Components**:
  * Unified Database class for storing document data and embeddings
  * Message broker for asynchronous task processing
  * Processes crawl jobs, chunks text, creates embeddings, fastapi service exposing endpoints
  * Docker and Docker Compose
```

## https://github.com/tysonthomas9/browser-operator-devtools-frontend

```
* Chromium browser with an user interface for running multi-agent workflows directly on the browser using a stateful, orchestration framework.
* Download the Agentic Browser for MacOS or set up the chromium dev tools with Agent framework on your system
* Built in Agent Framework for running tasks / workflows with actions such as:
    + Navigate URLs
    + Perform Actions (Clicks, Fill Form, Scroll)
    + Summarize content
    + Deep research topics
    + Literature reviews
    + Product comparisons
    + Shopping assistance
    + Advanced search
* Integrates with over 100+ LLM Models including OpenAI GPT-4.1, O4-Mini, Claude, Google Gemeni, and more
* Integrates with LiteLLM which supports multiple providers such as Huggingface, Groq, Azure, AWS
```
