<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>development on deskriders</title>
    <link>/categories/development/</link>
    <description>Recent content in development on deskriders</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 29 Dec 2024 10:10:13 +0000</lastBuildDate><atom:link href="/categories/development/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Setting Up Alnoda: A Dockerized Development Workspace</title>
      <link>/posts/1735467013-alnoda-workspaces/</link>
      <pubDate>Sun, 29 Dec 2024 10:10:13 +0000</pubDate>
      
      <guid>/posts/1735467013-alnoda-workspaces/</guid>
      <description>Alnoda desktop provides a dockerized development environment.
According to the GitHub repo
Open-source portable containerized workspaces. Isolate your work, make backups, copy, move between computers and cloud seamlessly.
You can start it up with the following command
docker run --name space-1 -d -p 8020-8040:8020-8040 --restart=always alnoda/alnoda-workspace Although due to port conflicts, I had to change to different port sequence (18020-18040).
docker run --name space-1 -d -p 18020-18040:8020-8040 --restart=always alnoda/alnoda-workspace Once it is up, go to http://localhost:18020 see the environment</description>
    </item>
    
    <item>
      <title>List of very handy uvx use-cases</title>
      <link>/posts/1734872892-uvx-example-usages/</link>
      <pubDate>Sun, 22 Dec 2024 13:08:12 +0000</pubDate>
      
      <guid>/posts/1734872892-uvx-example-usages/</guid>
      <description>Open Marimo uvx marimo edit --sandbox Also
uvx marimo tutorial intro iPython with packages uvx --with pandas,yfinance ipython Convert files to markdown uvx markitdown ssrn-4990063.pdf Aider.chat uvx aider Clean up .pyc files and pycache directories uvx pyclean . Collected from this Reddit thread
Run MLX VLM Models uv run --with mlx-vlm \ python -m mlx_vlm.generate \ --model Qwen/Qwen2-VL-2B-Instruct \ --max-tokens 1000 \ --temp 0.0 \ --image ~/Documents/Screenshots/2024/11/01/20241101_082217.png \ --prompt &amp;#34;Describe image in detail, include all text&amp;#34; uv run --with mlx-vlm --with torch \ python -m mlx_vlm.</description>
    </item>
    
  </channel>
</rss>
