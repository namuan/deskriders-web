<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="author" content="Deskriders.dev">
    <meta name="description" content="Improving developer productivity">
    <meta name="keywords" content="blog,developer">

    
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Running OpenAI Codex with locally running GPT OSS 20b model">
  <meta name="twitter:description" content="Llama.cpp Install llama.cpp
brew install llama.cpp Run llama-server
llama-server -hf ggml-org/gpt-oss-20b-GGUF --ctx-size 0 --jinja -ub 2048 -b 2048 --port 1234 The above command is suggested for devices less than 96GB RAM. See reference for other commands.
Codex brew install codex Setup config.toml file take ~/.codex vim ~/.codex/config.toml [model_providers.lms] name = &#34;LM Studio&#34; base_url = &#34;http://localhost:1234/v1&#34; [profiles.gpt-oss-20b-lms] model_provider = &#34;lms&#34; model = &#34;gpt-oss:20b&#34; Run codex
codex --profile gpt-oss-20b-lms Reference: https://github.com/ggml-org/llama.cpp/discussions/15396">

    <meta property="og:url" content="/posts/1759603064-running-codex-with-gpt-oss-20b/">
  <meta property="og:site_name" content="deskriders">
  <meta property="og:title" content="Running OpenAI Codex with locally running GPT OSS 20b model">
  <meta property="og:description" content="Llama.cpp Install llama.cpp
brew install llama.cpp Run llama-server
llama-server -hf ggml-org/gpt-oss-20b-GGUF --ctx-size 0 --jinja -ub 2048 -b 2048 --port 1234 The above command is suggested for devices less than 96GB RAM. See reference for other commands.
Codex brew install codex Setup config.toml file take ~/.codex vim ~/.codex/config.toml [model_providers.lms] name = &#34;LM Studio&#34; base_url = &#34;http://localhost:1234/v1&#34; [profiles.gpt-oss-20b-lms] model_provider = &#34;lms&#34; model = &#34;gpt-oss:20b&#34; Run codex
codex --profile gpt-oss-20b-lms Reference: https://github.com/ggml-org/llama.cpp/discussions/15396">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-10-04T19:37:45+01:00">
    <meta property="article:modified_time" content="2025-10-04T19:37:45+01:00">
    <meta property="article:tag" content="Llm">
    <meta property="article:tag" content="Codex">


    
      <base href="/posts/1759603064-running-codex-with-gpt-oss-20b/">
    
    <title>
  Running OpenAI Codex with locally running GPT OSS 20b model · deskriders
</title>

    
      <link rel="canonical" href="/posts/1759603064-running-codex-with-gpt-oss-20b/">
    

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700" rel="stylesheet">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.11.2/css/all.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="/css/coder.min.28d751104f30c16da1aa1bb04015cbe662cacfe0d1b01af4f2240ad58580069c.css" integrity="sha256-KNdREE8wwW2hqhuwQBXL5mLKz&#43;DRsBr08iQK1YWABpw=" crossorigin="anonymous" media="screen" />
    

    

    
      
        
        
        <link rel="stylesheet" href="/css/coder-dark.min.83a2010dac9f59f943b3004cd6c4f230507ad036da635d3621401d42ec4e2835.css" integrity="sha256-g6IBDayfWflDswBM1sTyMFB60DbaY102IUAdQuxOKDU=" crossorigin="anonymous" media="screen" />
      
    

    

    

    

    <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

    <meta name="generator" content="Hugo 0.143.1">
  </head>

  
  
    
  
  <body class="colorscheme-auto">
    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      deskriders
    </a>
    <input type="checkbox" id="menu-toggle" />
    <label class="menu-button float-right" for="menu-toggle"><i class="fas fa-bars"></i></label>
    <ul class="navigation-list">
      
        
          <li class="navigation-item">
            <a class="navigation-link" href="/posts/">Blog</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="/products">Products</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="/notes/">Notes</a>
          </li>
        
      
      
    </ul>
  </section>
</nav>


      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">Running OpenAI Codex with locally running GPT OSS 20b model</h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fas fa-calendar"></i>
              <time datetime='2025-10-04T19:37:45&#43;01:00'>
                October 4, 2025
              </time>
            </span>
            <span class="reading-time">
              <i class="fas fa-clock"></i>
              One minute read
            </span>
          </div>
          <div class="categories">
  <i class="fas fa-folder"></i>
    <a href="/categories/programming/">programming</a></div>

          <div class="tags">
  <i class="fas fa-tag"></i>
    <a href="/tags/llm/">llm</a>
      <span class="separator">•</span>
    <a href="/tags/codex/">codex</a></div>

        </div>
      </header>

      <div>
        <h2 id="llamacpp">Llama.cpp</h2>
<p>Install llama.cpp</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>brew install llama.cpp
</span></span></code></pre></div><p>Run <code>llama-server</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>llama-server -hf ggml-org/gpt-oss-20b-GGUF  --ctx-size <span style="color:#ae81ff">0</span> --jinja -ub <span style="color:#ae81ff">2048</span> -b <span style="color:#ae81ff">2048</span> --port <span style="color:#ae81ff">1234</span>
</span></span></code></pre></div><p>The above command is suggested for devices less than 96GB RAM. See reference for other commands.</p>
<h2 id="codex">Codex</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>brew install codex
</span></span></code></pre></div><h2 id="setup-configtoml-file">Setup config.toml file</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>take ~/.codex
</span></span><span style="display:flex;"><span>vim ~/.codex/config.toml
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-toml" data-lang="toml"><span style="display:flex;"><span>[<span style="color:#a6e22e">model_providers</span>.<span style="color:#a6e22e">lms</span>]
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">name</span> = <span style="color:#e6db74">&#34;LM Studio&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">base_url</span> = <span style="color:#e6db74">&#34;http://localhost:1234/v1&#34;</span>
</span></span><span style="display:flex;"><span>[<span style="color:#a6e22e">profiles</span>.<span style="color:#a6e22e">gpt-oss-20b-lms</span>]
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">model_provider</span> = <span style="color:#e6db74">&#34;lms&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">model</span> = <span style="color:#e6db74">&#34;gpt-oss:20b&#34;</span>
</span></span></code></pre></div><p>Run codex</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>codex --profile gpt-oss-20b-lms
</span></span></code></pre></div><h2 id="reference">Reference:</h2>
<p><a href="https://github.com/ggml-org/llama.cpp/discussions/15396">https://github.com/ggml-org/llama.cpp/discussions/15396</a></p>

      </div>

      <footer>
        


        
        
        
      </footer>
    </article>

    
  </section>

      </div>

      <footer class="footer">
  <section class="container">
    
    
      
        © 2019 - 2025
      
       Deskriders.dev 
    
    
       · 
      Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
    
    
  </section>
</footer>

    </main>

    

  </body>

</html>
