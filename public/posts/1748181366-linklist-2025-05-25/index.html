<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="author" content="Deskriders.dev">
    <meta name="description" content="Improving developer productivity">
    <meta name="keywords" content="blog,developer">

    
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Link List :: 2025-05-25">
  <meta name="twitter:description" content="https://github.com/rneatherway/gh-slack * This project provides a means of archiving a Slack conversation or thread as markdown. * For convenience, it is installable as a gh extension: `gh extension install https://github.com/rneatherway/gh-slack` and then upgrade with `gh extension upgrade gh-slack` https://github.com/automata/aicodeguide?tab=readme-ov-file#vibe This is an extensive guide on how to use Large Language Models (LLMs) for coding purposes. Here’s a summary of the main points:
Getting started with LLMs
Choose an LLM tool, such as Cursor Agent or Aider. Set up your development environment and project structure. Create a PRD (Product Requirements Document) to guide your LLM’s implementation. LLM Training and Guidelines">

    <meta property="og:url" content="/posts/1748181366-linklist-2025-05-25/">
  <meta property="og:site_name" content="deskriders">
  <meta property="og:title" content="Link List :: 2025-05-25">
  <meta property="og:description" content="https://github.com/rneatherway/gh-slack * This project provides a means of archiving a Slack conversation or thread as markdown. * For convenience, it is installable as a gh extension: `gh extension install https://github.com/rneatherway/gh-slack` and then upgrade with `gh extension upgrade gh-slack` https://github.com/automata/aicodeguide?tab=readme-ov-file#vibe This is an extensive guide on how to use Large Language Models (LLMs) for coding purposes. Here’s a summary of the main points:
Getting started with LLMs
Choose an LLM tool, such as Cursor Agent or Aider. Set up your development environment and project structure. Create a PRD (Product Requirements Document) to guide your LLM’s implementation. LLM Training and Guidelines">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-05-25T10:00:25+00:00">
    <meta property="article:modified_time" content="2025-05-25T10:00:25+00:00">
    <meta property="article:tag" content="Links">


    
      <base href="/posts/1748181366-linklist-2025-05-25/">
    
    <title>
  Link List :: 2025-05-25 · deskriders
</title>

    
      <link rel="canonical" href="/posts/1748181366-linklist-2025-05-25/">
    

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700" rel="stylesheet">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.11.2/css/all.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="/css/coder.min.28d751104f30c16da1aa1bb04015cbe662cacfe0d1b01af4f2240ad58580069c.css" integrity="sha256-KNdREE8wwW2hqhuwQBXL5mLKz&#43;DRsBr08iQK1YWABpw=" crossorigin="anonymous" media="screen" />
    

    

    
      
        
        
        <link rel="stylesheet" href="/css/coder-dark.min.83a2010dac9f59f943b3004cd6c4f230507ad036da635d3621401d42ec4e2835.css" integrity="sha256-g6IBDayfWflDswBM1sTyMFB60DbaY102IUAdQuxOKDU=" crossorigin="anonymous" media="screen" />
      
    

    

    

    

    <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

    <meta name="generator" content="Hugo 0.143.1">
  </head>

  
  
    
  
  <body class="colorscheme-auto">
    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      deskriders
    </a>
    <input type="checkbox" id="menu-toggle" />
    <label class="menu-button float-right" for="menu-toggle"><i class="fas fa-bars"></i></label>
    <ul class="navigation-list">
      
        
          <li class="navigation-item">
            <a class="navigation-link" href="/posts/">Blog</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="/products">Products</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="/notes/">Notes</a>
          </li>
        
      
      
    </ul>
  </section>
</nav>


      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">Link List :: 2025-05-25</h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fas fa-calendar"></i>
              <time datetime='2025-05-25T10:00:25Z'>
                May 25, 2025
              </time>
            </span>
            <span class="reading-time">
              <i class="fas fa-clock"></i>
              10 minutes read
            </span>
          </div>
          <div class="categories">
  <i class="fas fa-folder"></i>
    <a href="/categories/linklist/">linklist</a></div>

          <div class="tags">
  <i class="fas fa-tag"></i>
    <a href="/tags/links/">links</a></div>

        </div>
      </header>

      <div>
        <h2 id="httpsgithubcomrneatherwaygh-slack"><a href="https://github.com/rneatherway/gh-slack">https://github.com/rneatherway/gh-slack</a></h2>
<pre tabindex="0"><code>* This project provides a means of archiving a Slack conversation or thread as markdown.
* For convenience, it is installable as a gh extension: `gh extension install https://github.com/rneatherway/gh-slack` and then upgrade with `gh extension upgrade gh-slack`
</code></pre><h2 id="httpsgithubcomautomataaicodeguidetabreadme-ov-filevibe"><a href="https://github.com/automata/aicodeguide?tab=readme-ov-file#vibe">https://github.com/automata/aicodeguide?tab=readme-ov-file#vibe</a></h2>
<p>This is an extensive guide on how to use Large Language Models (LLMs) for coding purposes. Here&rsquo;s a summary of the main points:</p>
<p><strong>Getting started with LLMs</strong></p>
<ol>
<li>Choose an LLM tool, such as Cursor Agent or Aider.</li>
<li>Set up your development environment and project structure.</li>
<li>Create a PRD (Product Requirements Document) to guide your LLM&rsquo;s implementation.</li>
</ol>
<p><strong>LLM Training and Guidelines</strong></p>
<ol>
<li>Train your LLM using a starter kit or existing codebase.</li>
<li>Use tools like repomix, files-to-prompt, or indexing features to optimize your LLM&rsquo;s performance.</li>
<li>Prompt-engineer your design sequence to produce well-structured code.</li>
</ol>
<p><strong>Tips for Effective LLM Coding</strong></p>
<ol>
<li>Separate engine and policy code, and prompt the LLM to guide it.</li>
<li>Include rules in your project that prevent layering violations.</li>
<li>Start with core functionality and organize your code structure before exposing it as a library or API.</li>
<li>Write tests and use TDD (Test-Driven Development) to create test-driven skeletons.</li>
</ol>
<p><strong>Testing and Validation</strong></p>
<ol>
<li>Regularly check the generated code for correctness and consistency.</li>
<li>Use tests to validate the LLM&rsquo;s output and ensure that it meets your requirements.</li>
</ol>
<p><strong>Tools and Resources</strong></p>
<ol>
<li>Keep an updated list of main tools around using AI for coding.</li>
<li>Check out the work of influential individuals, such as Andrej Karpathy, Geoffrey Huntley, and Eric S. Raymond.</li>
</ol>
<h2 id="httpscloudgooglecomrundocstipspython"><a href="https://cloud.google.com/run/docs/tips/python">https://cloud.google.com/run/docs/tips/python</a></h2>
<pre tabindex="0"><code>* Optimizations for Python-based Cloud Run services include handling concurrent requests and reducing response latency using connection pooling and batching non-critical functions.
* Optimize the container image by:
    + Only putting necessary components into the container at runtime
    + Using a smaller base image (e.g. `python:slim` or `distroless`)
* Minimize the container image by:
    + Moving large files outside of the container to Cloud Storage
    + Adding specific packages back in using `RUN apt install`
* Optimize the WSGI server by:
    + Using a smaller number of workers and threads (e.g. with gunicorn&#39;s `--preload` setting)
    + Setting up custom startup probes for your service
    + Considering alternative WSGI servers like uwsgi, uvicorn, or waitress
* Optimize applications by:
    + Reducing threads to optimize memory usage
    + Decreasing the number of startup tasks to reduce cold start latency
* Consider using instance-based billing to run background activities outside of requests.
</code></pre><h2 id="httpsgithubcomhichemtab-techjettreemark"><a href="https://github.com/HichemTab-tech/JetTreeMark">https://github.com/HichemTab-tech/JetTreeMark</a></h2>
<pre tabindex="0"><code>- Generate a clean tree view of any selected folder
- One-click copy to clipboard
- Flexible node control with context menu options:
  - Check only folders or only files
  - Check nodes without affecting their children
  - Expand or collapse all nodes
- Ultra lightweight and seamless integration with the IDE
</code></pre><h2 id="httpsgithubcomrockbitelocalforge"><a href="https://github.com/rockbite/localforge">https://github.com/rockbite/localforge</a></h2>
<pre tabindex="0"><code>* Local coding agent with neat UI
* Runs in a local web UI and autonomously works on your files alongside you.
* `npm i -g @rockbite/localforge` to install globally
* Install dependencies: `npm install`
* Start development server: `npm run start`
* Main guide: https://localforge.dev/how-to
</code></pre><h2 id="httpsarxivorgabs250416736"><a href="https://arxiv.org/abs/2504.16736">https://arxiv.org/abs/2504.16736</a></h2>
<pre tabindex="0"><code>* A Survey of AI Agent Protocols
* Abstract:The rapid development of large language models (LLMs) has led to the widespread deployment of LLM agents across diverse industries.
* However, as more LLM agents are deployed, a major issue has emerged: there is no standard way for these agents to communicate with external tools or data sources.
* This lack of standardized protocols makes it difficult for agents to work together or scale effectively.
* A unified communication protocol for LLM agents could change this.
* It would allow agents and tools to interact more smoothly, encourage collaboration, and triggering the formation of collective intelligence.
* In this paper, we provide the first comprehensive analysis of existing agent protocols.
* We propose a systematic two-dimensional classification that differentiates context-oriented versus inter-agent protocols and general-purpose versus domain-specific protocols.
* Additionally, we conduct a comparative performance analysis of these protocols across key dimensions such as security, scalability, and latency.
* Finally, we explore the future landscape of agent protocols by identifying critical research directions and characteristics necessary for next-generation protocols.
* These characteristics include adaptability, privacy preservation, and group-based interaction.
* We also identify trends toward layered architectures and collective intelligence infrastructures.
* We expect this work to serve as a practical reference for both researchers and engineers seeking to design, evaluate, or integrate robust communication infrastructures for intelligent agents.
</code></pre><h2 id="httpsgithubcomn8n-ioself-hosted-ai-starter-kit"><a href="https://github.com/n8n-io/self-hosted-ai-starter-kit">https://github.com/n8n-io/self-hosted-ai-starter-kit</a></h2>
<pre tabindex="0"><code>* Self-hosted AI Starter Kit is an open-source Docker Compose template designed to swiftly initialize a comprehensive local AI and low-code development environment.
* Curated by https://github.com/n8n-io, it combines the self-hosted n8n platform with a curated list of compatible AI products and components.
* Key components:
    * Self-hosted n8n - Low-code platform with over 400 integrations and advanced AI components
    * Ollama - Cross-platform LLM platform to install and run the latest local LLMs
    * Qdrant - Open-source, high performance vector store with a comprehensive API
    * PostgreSQL - Workhorse of the Data Engineering world, handles large amounts of data safely.
* Features:
    * AI Agents for scheduling appointments
    * Summarize Company PDFs securely without data leaks
    * Smarter Slack Bots for enhanced company communications and IT operations
    * Private Financial Document Analysis at minimal cost
* Installation steps:
    * Clone the repository and navigate to the directory
    * Run `docker compose --profile gpu-nvidia up` (or `--profile cpu up`) depending on GPU availability
    * Configure OLLAMA_HOST environment variable if running locally
* Getting started:
    * Open http://localhost:5678/ in browser to set up n8n
    * Open included workflow and start running the workflow
    * Use Ollama node for language model and Qdrant as vector store
* Customization and optimization:
    * Note that this starter kit is not fully optimized for production environments
    * Can be customized to meet specific needs
* Resources:
    * n8n AI template gallery for more workflow ideas
    * Support page for troubleshooting and community discussion
</code></pre><h2 id="httpsgithubcomshobrooksuss"><a href="https://github.com/shobrook/suss">https://github.com/shobrook/suss</a></h2>
<pre tabindex="0"><code>* Find bugs in local code changes with a codebase-aware AI agent, analyzing diff between local and remote branches.
* Run `suss` in terminal to get bug report in under 1 minute.
* Works by exploring codebase context for each code change and evaluating it using a reasoning model.
* Can catch complex, multi-file bugs and is useful as a quick code review before pushing code.
* Install with `pipx install suss`.
* Choose LLM provider (e.g. OpenAI, Anthropic) and add API key to environment variables.
* Use other models or local ones, including Cohere reranker model.
* Boost performance using Cohere API key.
* Run `suss` in root directory of codebase by default analyzing new or modified files.
* Run `suss --file=&#34;path/to/file.py&#34;` for specific file analysis.
</code></pre><h2 id="httpsblogprateekjaindeva-step-by-step-guide-to-docker-image-optimisation-reduce-size-by-over-95-d90bcab3819d"><a href="https://blog.prateekjain.dev/a-step-by-step-guide-to-docker-image-optimisation-reduce-size-by-over-95-d90bcab3819d">https://blog.prateekjain.dev/a-step-by-step-guide-to-docker-image-optimisation-reduce-size-by-over-95-d90bcab3819d</a></h2>
<pre tabindex="0"><code>* A Step-by-Step Guide to Docker Image Optimisation: Reduce Size by Over 95%
* Deployed a Node.js app to production using Docker, resulting in a 1.2 GB final image size
* Oversized images slowed down deployments, hiked up costs, and increased attack surface
* Techniques found to shrink Docker images by up to 95% through trial-and-error
* Image size affects application behavior in production:
    * Faster Deployments: Smaller images move through CI/CD pipelines faster
    * Reduced Costs: Lean images consume less storage and resources
    * Impact on Infrastructure Costs
</code></pre><h2 id="httpsgithubcomplasma-umasschatdbg"><a href="https://github.com/plasma-umass/ChatDBG">https://github.com/plasma-umass/ChatDBG</a></h2>
<pre tabindex="0"><code>The ChatDBG documentation provides an overview of how to use the tool for debugging various programming languages, including Rust.
</code></pre><h2 id="httpsgithubcomaperoctoolkami"><a href="https://github.com/aperoc/toolkami">https://github.com/aperoc/toolkami</a></h2>
<pre tabindex="0"><code>Minimal AI agent framework that just works with only seven tools.
* Seven tools are required: 
  * Minimal AI agent
  * Turbo mode
  * Hot-reloading
  * Devcontainer with UV installation instructions
  * Servers/.env and clients/.env files for credentials
  * MCP server start command
  * MCP client start command
  * Command to clear message history

* MCP tool:
  * Minimal implementation of AlphaEvolve using the framework
  * Diff-fenced editing
  * Credits to https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/
  
* Additional features and tools:
  * System prompt guidelines
  * Single file project templates
  * OpenAI editing format
  * Customisable sharp tool (with guardrails to be implemented over time)
</code></pre><h2 id="httpsgithubcommaverickg59sushimcp"><a href="https://github.com/maverickg59/sushimcp">https://github.com/maverickg59/sushimcp</a></h2>
<pre tabindex="0"><code>* SushiMCP is a model context protocol server designed to assist developers with delivering context to their AI IDE&#39;s.
* It improves the performance of base and premium LLM models when generating code.
* The easiest way to get started is by registering SushiMCP with your client using the default configuration.
* The default configuration uses:
  * `npx` command
  * `@chriswhiterocks/sushimcp` module
  * `--llms-txt-source` and `--openapi-spec-source` flags
* Visit the [SushiMCP Docs](link) for more information on advanced configuration.
* The project is licensed under the AGPL-3.0-or-later license.
* See the [license.txt](file) for details about the license.
</code></pre><h2 id="httpsoldredditcomrlocalllamacomments1khjrtjbuilding_llm_workflows_some_observations"><a href="https://old.reddit.com/r/LocalLLaMA/comments/1khjrtj/building_llm_workflows_some_observations/">https://old.reddit.com/r/LocalLLaMA/comments/1khjrtj/building_llm_workflows_some_observations/</a></h2>
<pre tabindex="0"><code>* Decomposing tasks into smallest steps and prompt chaining works better than using a single prompt with CoT
* Using XML tags for system prompt, prompt etc is preferred over JSON structure (IMO)
* LLM should be reminded of its role as a semantic parser, without introducing data from its own &#34;knowledge&#34;
* NLTK, SpaCY, FlairNLP are good for verifying output of an LLM independently
* ModernBERT classifiers are often just as good as LLMs for small tasks
* Fine-tuned BERT-style classifiers are usually better than LLM for focused narrow tasks
* LLM-as-judge and confidence scoring is unreliable, especially without &#34;grounding&#34; parameters
* Scoring on vague parameters like helpfulness is useless - requires grounding or fine-tuning model
* Agentic loops require setting conditions for LLM to exit loop, which is unreliable
* Performance usually degrades past 4k tokens (input context window)
* 32B models are good enough and reliable enough for most tasks
* Structured CoT with headings and bullet points is often better than unstructured CoT tokens
* Self-consistency helps but requires running each prompt multiple times, forcing smaller models and prompts
* Writing own CoT is better than relying on reasoning model for ideas synthesis
* Long-term plan is to fine-tune everything, starting with large API-based model and few-shot examples
* Fine-tuning datasets should be balanced by setting up a categorization system/orthogonal taxonomy
</code></pre><h2 id="httpsgithubcommarcboekergmail-to-sqlite"><a href="https://github.com/marcboeker/gmail-to-sqlite">https://github.com/marcboeker/gmail-to-sqlite</a></h2>
<pre tabindex="0"><code>Index your Gmail account to a SQLite DB and play with the data.
</code></pre><h2 id="httpswwwpewresearchorgshort-reads20240305online-opt-in-polls-can-produce-misleading-results-especially-for-young-people-and-hispanic-adults"><a href="https://www.pewresearch.org/short-reads/2024/03/05/online-opt-in-polls-can-produce-misleading-results-especially-for-young-people-and-hispanic-adults/">https://www.pewresearch.org/short-reads/2024/03/05/online-opt-in-polls-can-produce-misleading-results-especially-for-young-people-and-hispanic-adults/</a></h2>
<pre tabindex="0"><code>* At Pew Research Center, we&#39;ve analyzed the strengths and weaknesses of polling methods, including online &#34;opt-in&#34; sampling, which can struggle to measure attitudes among young people and Hispanics due to the presence of &#34;bogus respondents.&#34;
* Bogus respondents attempt to complete surveys with minimal effort to earn rewards and may overreport rare attitudes or behaviors.
* Studies have shown that bogus respondents are more concentrated in estimates for adults under 30 and Hispanic adults, leading to unreliable estimates for these groups.
* A recent online opt-in survey reported a startling finding about Holocaust denial among young Americans, which we replicated using a probability-based survey panel.
* Our findings showed that the original opt-in poll&#39;s estimates were significantly different from ours, suggesting that bogus respondents may have driven the results.
* The replication also found substantial differences in support for legal abortion between the two surveys, supporting our expectation that bogus respondents would be responsible for these discrepancies.
* The study highlights the importance of using rigorous survey methods to avoid inadvertently misleading the public, particularly when studying complex and sensitive topics like antisemitism and the Middle East.
</code></pre><h2 id="httpsgithubcomcollidingscopesshape-creator-tutorial"><a href="https://github.com/collidingScopes/shape-creator-tutorial">https://github.com/collidingScopes/shape-creator-tutorial</a></h2>
<pre tabindex="0"><code>* The demo uses:
  + Three.js for 3D rendering
  + MediaPipe for hand tracking and gesture recognition
  + HTML5 Canvas for visual feedback
  + JavaScript for real-time interaction
</code></pre><h2 id="httpsgithubcomsisig-aidoctor"><a href="https://github.com/sisig-ai/doctor">https://github.com/sisig-ai/doctor</a></h2>
<pre tabindex="0"><code>- **Overview**: Tool for discovering, crawling and indexing web sites to be exposed as an MCP server for LLM agents
- **Doctor&#39;s capabilities**:
  * Crawling web pages using crawl4ai with hierarchy tracking
  * Chunking text with LangChain
  * Creating embeddings with OpenAI via litellm
  * Storing data in DuckDB with vector search support
  * Exposing search functionality via a FastAPI web service
- **Components**:
  * Unified Database class for storing document data and embeddings
  * Message broker for asynchronous task processing
  * Processes crawl jobs, chunks text, creates embeddings, fastapi service exposing endpoints
  * Docker and Docker Compose
</code></pre><h2 id="httpsgithubcomtysonthomas9browser-operator-devtools-frontend"><a href="https://github.com/tysonthomas9/browser-operator-devtools-frontend">https://github.com/tysonthomas9/browser-operator-devtools-frontend</a></h2>
<pre tabindex="0"><code>* Chromium browser with an user interface for running multi-agent workflows directly on the browser using a stateful, orchestration framework.
* Download the Agentic Browser for MacOS or set up the chromium dev tools with Agent framework on your system
* Built in Agent Framework for running tasks / workflows with actions such as:
    + Navigate URLs
    + Perform Actions (Clicks, Fill Form, Scroll)
    + Summarize content
    + Deep research topics
    + Literature reviews
    + Product comparisons
    + Shopping assistance
    + Advanced search
* Integrates with over 100+ LLM Models including OpenAI GPT-4.1, O4-Mini, Claude, Google Gemeni, and more
* Integrates with LiteLLM which supports multiple providers such as Huggingface, Groq, Azure, AWS
</code></pre>
      </div>

      <footer>
        


        
        
        
      </footer>
    </article>

    
  </section>

      </div>

      <footer class="footer">
  <section class="container">
    
    
      
        © 2019 - 2025
      
       Deskriders.dev 
    
    
       · 
      Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
    
    
  </section>
</footer>

    </main>

    

  </body>

</html>
